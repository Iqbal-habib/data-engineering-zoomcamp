**Objective: The goal will be to construct an ETL pipeline that loads the data, performs some transformations, and writes the data to a database (and Google Cloud!).**

Instructions:
- Need to load the green taxi data:
- `https://github.com/DataTalksClub/nyc-tlc-data/releases/tag/green/download`
- You may need to reference the link below to download via Python in Mage:
- `https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/`

**Part 1  - Load the data into Mage**
Instructions:
- Create a new pipeline called `green_taxi_etl` 
- Create a data loader block to reach data for the 2020 months 10,11,12
- Copying the specific links that are required below:
	- url = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2020-10.csv.gz'
	- url = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2020-11.csv.gz'
	- url = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2020-12.csv.gz'
- Next we must define the data types (this was given to us in the video):
```python
taxi_dtypes = {

				'VendorID': pd.Int64Dtype(),
				'passenger_count': pd.Int64Dtype(),
				'trip_distance': float,
				'RatecodeID':pd.Int64Dtype(),
				'store_and_fwd_flag':str,
				'PULocationID':pd.Int64Dtype(),
				'DOLocationID':pd.Int64Dtype(),
				'payment_type': pd.Int64Dtype(),
				'fare_amount': float,
				'extra':float,
				'mta_tax':float,
				'tip_amount':float,
				'tolls_amount':float,
				'improvement_surcharge':float,
				'total_amount':float,
				'congestion_surcharge':float
			}
```
- Interestingly when I use get.schema in jupyter notebook to get the DDL, I get slightly different results:
```python
CREATE TABLE "green_trip_data_schema" (
										"VendorID" INTEGER,
										  "lpep_pickup_datetime" TEXT,
										  "lpep_dropoff_datetime" TEXT,
										  "store_and_fwd_flag" TEXT,
										  "RatecodeID" INTEGER,
										  "PULocationID" INTEGER,
										  "DOLocationID" INTEGER,
										  "passenger_count" INTEGER,
										  "trip_distance" REAL,
										  "fare_amount" REAL,
										  "extra" REAL,
										  "mta_tax" REAL,
										  "tip_amount" REAL,
										  "tolls_amount" REAL,
										  "ehail_fee" REAL,
										  "improvement_surcharge" REAL,
										  "total_amount" REAL,
										  "payment_type" INTEGER,
										  "trip_type" INTEGER,
										  "congestion_surcharge" REAL
									)
```

- The reason for this is that `int64` is a pandas expression of a data type, while `INTEGER` is a python expression
- Another interesting thing is that he uses `pd.Int64Dtype()`  as this can handle nullable integer data (NaN values), allowing for the representation of missing values.

Note to self: Why do we need pandas?
- open-source data manipulation and analysis library for the Python programming language. It provides data structures for efficiently storing and manipulating large datasets, as well as tools for working with structured data.

```python
#this loads the data using pandas and the concat function.

import io
import pandas as pd
import requests
if 'data_loader' not in globals():
	from mage_ai.data_preparation.decorators import data_loader
if 'test' not in globals():
	from mage_ai.data_preparation.decorators import test

@data_loader
def load_data_from_api(*args, **kwargs):
	
	url1 = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2020-10.csv.gz'
	month_1 = pd.read_csv(url1, compression='gzip', low_memory=False)
	
	url2 = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2020-11.csv.gz'
	month_2 = pd.read_csv(url2, compression='gzip', low_memory=False)
	
	url3 = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2020-12.csv.gz'
	month_3 = pd.read_csv(url3, compression='gzip', low_memory=False)
	
	df = pd.concat([month_1, month_2, month_3], ignore_index=True)

	taxi_dtypes = {
					'VendorID': pd.Int64Dtype(),
					'passenger_count': pd.Int64Dtype(),
					'trip_distance': float,
					'RatecodeID':pd.Int64Dtype(),
					'store_and_fwd_flag':str,
					'PULocationID':pd.Int64Dtype(),
					'DOLocationID':pd.Int64Dtype(),
					'payment_type': pd.Int64Dtype(),
					'fare_amount': float,
					'extra':float,
					'mta_tax':float,
					'tip_amount':float,
					'tolls_amount':float,
					'improvement_surcharge':float,
					'total_amount':float,
					'congestion_surcharge':float
					}

# native date parsing
	parse_dates = ['lpep_pickup_datetime', 'lpep_dropoff_datetime']
	return df.astype(taxi_dtypes) # Use astype to set data types
	 
@test
def test_output(output, *args) -> None:
	assert output is not None, 'The output is undefined'
```

**Part 2  - Transform the data (in Mage)

```python

#remove passenger counts and trip distances that are less than 0
#rename all the camel case columns
#add in a new column for pickup date (that doesn't have the time)

import pandas as pd
if 'transformer' not in globals():
	from mage_ai.data_preparation.decorators import transformer
if 'test' not in globals():
	from mage_ai.data_preparation.decorators import test

@transformer
def transform(data, *args, **kwargs):
	df=data[data['passenger_count'] > 0]
	df=df[df['trip_distance']>0]
	df['lpep_pickup_date'] = pd.to_datetime(df['lpep_pickup_datetime']).dt.date
	df = df.rename(columns={'VendorID': 'vendor_id', 'RatecodeID': 'rate_code_id', 'PULocationID': 'pu_location_id', 'DOLocationID': 'do_location_id'})

	return df
	
@test
def test_output(output, *args) -> None:

	assert output is not None, 'The output is undefined'
	assert (output['passenger_count']> 0).all(), 'There are no rides with zero passengers'
	assert (output['trip_distance'] > 0).all(), 'There are no trips with a distance of less than 0'
	assert output['vendor_id'].isna().sum() == 0
```

**Part 3  - Export the data into postgress

```python
from mage_ai.settings.repo import get_repo_path
from mage_ai.io.config import ConfigFileLoader
from mage_ai.io.postgres import Postgres
from pandas import DataFrame
from os import path

if 'data_exporter' not in globals():
	from mage_ai.data_preparation.decorators import data_exporter
@data_exporter
def export_data_to_postgres(df: DataFrame, **kwargs) -> None:

	schema_name = 'mage' # Specify the name of the schema to export data to
	table_name = 'green_taxi' # Specify the name of the table to export data to
	config_path = path.join(get_repo_path(), 'io_config.yaml')
	config_profile = 'dev'
	with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:
loader.export(
			df,
			schema_name,
			table_name,
			index=False, # Specifies whether to include index in exported table
			if_exists='replace', # Specify resolution policy if table name already exists
			)
```

**Part 4  - Export the data into gcs

```python
import pyarrow as pa
import pyarrow.parquet as pq
import os
import pandas as pd

if 'data_exporter' not in globals():
	from mage_ai.data_preparation.decorators import data_exporter

	os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = "/home/src/optimal-pursuit-412822-5179d8d770a4.json"
	bucket_name = 'mage-zoomcamp-zay'
	project_id = 'optimal-pursuit-412822'
	table_name = "green_taxi"
	root_path = f'{bucket_name}/{table_name}'

@data_exporter
def export_data(data, *args, **kwargs):
		data = pd.DataFrame(data)
		data_types = data.dtypes
		print(data_types)
		table = pa.Table.from_pandas(data)
		gcs = pa.fs.GcsFileSystem()
		pq.write_to_dataset(
		table,
		root_path=root_path,
		partition_cols=['lpep_pickup_date'],
		filesystem=gcs
		)

```

## Question 1. Data Loading

Once the dataset is loaded, what's the shape of the data?

- 266,855 rows x 20 columns
- 544,898 rows x 18 columns
- 544,898 rows x 20 columns
- 133,744 rows x 20 columns

Answer:
- 266,855 rows x 20 columns

## Question 2. Data Transformation

Upon filtering the dataset where the passenger count is greater than 0 _and_ the trip distance is greater than zero, how many rows are left?

- 544,897 rows
- 266,855 rows
- 139,370 rows
- 266,856 rows

Answer:
- 139,370 rows

##   Question 3. Data Transformation

Which of the following creates a new column `lpep_pickup_date` by converting `lpep_pickup_datetime` to a date?

- `data = data['lpep_pickup_datetime'].date`
- `data('lpep_pickup_date') = data['lpep_pickup_datetime'].date`
- `data['lpep_pickup_date'] = data['lpep_pickup_datetime'].dt.date`
- `data['lpep_pickup_date'] = data['lpep_pickup_datetime'].dt().date()`

Answer:
- `data['lpep_pickup_date'] = data['lpep_pickup_datetime'].dt.date`

## Question 4. Data Transformation

What are the existing values of `VendorID` in the dataset?

- 1, 2, or 3
- 1 or 2
- 1, 2, 3, 4
- 1

Answer:
- 

## [](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/cohorts/2024/02-workflow-orchestration/homework.md#question-5-data-transformation)Question 5. Data Transformation

How many columns need to be renamed to snake case?

- 3
- 6
- 2
- 4

Answer:
- 4

## [](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/cohorts/2024/02-workflow-orchestration/homework.md#question-6-data-exporting)Question 6. Data Exporting

Once exported, how many partitions (folders) are present in Google Cloud?

- 96
- 56
- 67
- 108

Answer:
- 95 lol!